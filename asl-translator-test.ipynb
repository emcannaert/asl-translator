{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32814c4-30cc-451c-ba8d-1ff4f09cb997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as transform\n",
    "from torch.nn.functional import normalize\n",
    "from PIL import Image\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import albumentations\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb0ca6-723d-49a8-8d3c-592a0d3d42b0",
   "metadata": {},
   "source": [
    "### 1. Get training dataset in usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a25c2-595e-4037-b053-62879bc9096b",
   "metadata": {},
   "source": [
    "- images are in jpg format, need to convert these to PyTorch tensors\n",
    "- tensors also have to be normalized  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cd005-68e1-4e32-bd1b-c47b3bccb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchvision.io.read_image()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2261e9bc-79e9-4a1a-8312-51a60ec45ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### class to load and store relevant data \n",
    "class LoadImageData:   \n",
    "    def __init__(self, n_files_per_class=10000):  ### n_files == number of files of each letter to train on\n",
    "        self.label_converter = dict()\n",
    "        self.convert_label_to_int()\n",
    "        self.n_files_per_class = n_files_per_class\n",
    "        self.train_data    = [] # train data paths, no longer in tensor format\n",
    "        self.test_data     = [] # test data paths, no longer in tensor format\n",
    "        self.train_labels = [] # truth LETTER label for training data\n",
    "        self.test_labels   = [] # truth LETTER label for test data\n",
    "        self.train_labels_num =  [] # truth NUMBER label for training data\n",
    "        self.test_labels_num  =  [] # truth NUMBER label for test data\n",
    "        self.image_path_dict = dict() # dictionary of all training file paths\n",
    "        self.load_train_data()\n",
    "        self.load_test_data()\n",
    "        \n",
    "        self.transf = albumentations.Compose([\n",
    "            albumentations.Resize(224, 224, always_apply=True),\n",
    "        ])\n",
    "    ### helper function to convert str letters/del/space into numbers \n",
    "    def convert_label_to_int(self):\n",
    "        alphabet = \"A/B/C/D/E/F/G/H/I/J/K/L/M/N/O/P/Q/R/S/T/U/V/W/X/Y/Z/del/nothing/space\"\n",
    "        for iii,label in enumerate(alphabet.split(\"/\")):\n",
    "            self.label_converter[label] = iii\n",
    "    \n",
    "    ### load training data into the instance variable train_data\n",
    "    def load_train_data(self):\n",
    "        now = time.time()\n",
    "        train_path = \"datasets/asl_alphabet_train/asl_alphabet_train/\"\n",
    "        train_directories = glob.glob(train_path+\"/*\")\n",
    "        n_files = 0\n",
    "        print(\" ----- Loading training dataset -----\")\n",
    "\n",
    "        for dir in train_directories:\n",
    "            letter = dir.split(\"/\")[-1]\n",
    "            self.image_path_dict[letter] = []\n",
    "            n_test_for_class = 0 \n",
    "            for image_file in glob.glob(dir+\"/*\"):\n",
    "                if n_test_for_class > (self.n_files_per_class-1):\n",
    "                    break ### move onto the next letter \n",
    "                self.image_path_dict[letter].append(image_file)\n",
    "                self.train_data.append( image_file )\n",
    "                self.train_labels_num.append(self.label_converter[letter])\n",
    "                self.train_labels.append(letter)\n",
    "                n_test_for_class+=1\n",
    "                n_files +=1\n",
    "            print(\"Finished importing %s\"%letter)\n",
    "        print(\"Done with training dataset - loaded paths for %i files. Took %f seconds\"%(n_files, np.around(time.time()-now)))\n",
    "        return\n",
    "    ### load test data into the instance variable train_data\n",
    "    def load_test_data(self):\n",
    "        now = time.time()\n",
    "        test_path = \"datasets/asl_alphabet_test/asl_alphabet_test/\"\n",
    "        test_directories = glob.glob(test_path+\"/*\")\n",
    "        n_files = 0\n",
    "        print(\"----- Loading test dataset -----\")\n",
    "        for image_file in test_directories:\n",
    "            letter = image_file.split(\"_\")[-2].split(\"/\")[-1]\n",
    "            self.image_path_dict[letter] = []\n",
    "            self.image_path_dict[letter].append(image_file)\n",
    "            self.test_data.append(image_file   )\n",
    "            self.test_labels_num.append(self.label_converter[letter])\n",
    "            self.test_labels.append(letter)\n",
    "            n_files +=1\n",
    "        print(\"Done with test dataset - loaded paths for %i files. Took %f seconds\"%(n_files, np.around(time.time()-now,4)))\n",
    "        return\n",
    "    def __getitem__(self,i):\n",
    "        image = cv2.imread(self.train_data[i])\n",
    "        image = self.transf(image=np.array(image))['image']\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        label = self.train_labels[i]\n",
    "        return torch.tensor(image, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
    "### lighter class to store test/train paths and labels\n",
    "class ImageData:\n",
    "    def __init__(self,paths, labels):\n",
    "        self.X = paths\n",
    "        self.y = labels\n",
    "        self.transf = albumentations.Compose([\n",
    "            albumentations.Resize(224, 224, always_apply=True),\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return (len(self.X))\n",
    "        \n",
    "    def __getitem__(self,i):\n",
    "        image = cv2.imread(self.X[i])\n",
    "        image = self.transf(image=np.array(image))['image']\n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        label = self.y[i]\n",
    "        return torch.tensor(image, dtype=torch.float), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b581c-adcc-49f6-a82f-eb8415b77ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c80220e5-acf4-4c78-9ff5-d6845bcb03fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----- Loading training dataset -----\n",
      "Finished importing H\n",
      "Finished importing N\n",
      "Finished importing E\n",
      "Finished importing Q\n",
      "Finished importing B\n",
      "Finished importing I\n",
      "Finished importing U\n",
      "Finished importing Y\n",
      "Finished importing G\n",
      "Finished importing K\n",
      "Finished importing D\n",
      "Finished importing del\n",
      "Finished importing M\n",
      "Finished importing A\n",
      "Finished importing L\n",
      "Finished importing J\n",
      "Finished importing X\n",
      "Finished importing W\n",
      "Finished importing R\n",
      "Finished importing V\n",
      "Finished importing F\n",
      "Finished importing Z\n",
      "Finished importing P\n",
      "Finished importing space\n",
      "Finished importing nothing\n",
      "Finished importing T\n",
      "Finished importing C\n",
      "Finished importing S\n",
      "Finished importing O\n",
      "Done with training dataset - loaded paths for 29000 files. Took 0.000000 seconds\n",
      "----- Loading test dataset -----\n",
      "Done with test dataset - loaded paths for 28 files. Took 0.000200 seconds\n"
     ]
    }
   ],
   "source": [
    "datasets = LoadImageData(1000) ## data instance, passing in 1000 so that only 1000 of each letter are used for training \n",
    "### changing gears, keeping the \"data\" as the file paths to each jpg, also imbuing this class with a __get__\n",
    "### built-in method that returns the relevent tensors for when these are needed \n",
    "\n",
    "train_data = ImageData(datasets.train_data, datasets.train_labels_num)\n",
    "test_data  = ImageData(datasets.test_data, datasets.test_labels_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "222cf2ba-d5ea-4b3a-8f78-0eb3d25a3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 \n",
    "### now load data into ptytorch\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abec70-c546-4235-bf06-016c83cc9c2b",
   "metadata": {},
   "source": [
    "### 2. Create a csv with the file path, corresponding, letter, and then binarize this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccc45c35-1672-4af6-a6d1-f7bd9fd26e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.DataFrame()\n",
    "df_training['path'] = \"\"\n",
    "df_training['letter'] = \"\"\n",
    "for iii in range(0,len(id_.train_data)):\n",
    "    df_training.loc[iii, 'path' ] = id_.train_data[iii]\n",
    "    df_training.loc[iii, 'letter' ] = id_.train_labels[iii] ### converting letter to int\n",
    "df_training = df_training.sample(frac=1).reset_index(drop=True) ### shuffle\n",
    "df_training.to_csv(\"processedDatasets/train_data.csv\") ### write out csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0f1071f-843b-4670-8912-60a4d392a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_binarized = pd.get_dummies(df_training[\"letter\"],dtype=int) ### binarize\n",
    "letters_binarized.insert(0, 'path', df_training['path']) ### reinsert the path \n",
    "letters_binarized.to_csv(\"processedDatasets/train_data_binarized.csv\") ### write out binarized csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f75ba6a-0908-4853-8bd1-5875ee6f22d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>...</th>\n",
       "      <th>T</th>\n",
       "      <th>U</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>del</th>\n",
       "      <th>nothing</th>\n",
       "      <th>space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>datasets/asl_alphabet_train/asl_alphabet_train...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  A  B  C  D  E  F  G  H  \\\n",
       "0  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  0  0   \n",
       "1  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  0  1   \n",
       "2  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  1  0   \n",
       "3  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  0  0   \n",
       "4  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  0  0   \n",
       "5  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  0  0   \n",
       "6  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  0  0   \n",
       "7  datasets/asl_alphabet_train/asl_alphabet_train...  1  0  0  0  0  0  0  0   \n",
       "8  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  0  0   \n",
       "9  datasets/asl_alphabet_train/asl_alphabet_train...  0  0  0  0  0  0  0  0   \n",
       "\n",
       "   I  ...  T  U  V  W  X  Y  Z  del  nothing  space  \n",
       "0  0  ...  0  0  0  0  0  0  0    0        0      0  \n",
       "1  0  ...  0  0  0  0  0  0  0    0        0      0  \n",
       "2  0  ...  0  0  0  0  0  0  0    0        0      0  \n",
       "3  0  ...  0  0  1  0  0  0  0    0        0      0  \n",
       "4  0  ...  0  0  0  1  0  0  0    0        0      0  \n",
       "5  0  ...  0  0  0  0  0  0  0    0        0      0  \n",
       "6  0  ...  0  0  0  0  0  0  0    1        0      0  \n",
       "7  0  ...  0  0  0  0  0  0  0    0        0      0  \n",
       "8  0  ...  0  0  0  1  0  0  0    0        0      0  \n",
       "9  0  ...  1  0  0  0  0  0  0    0        0      0  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters_binarized.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f4e7e",
   "metadata": {},
   "source": [
    "### 2.5 Save labels in a loadable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1dda4c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n### test pickle file is working\\nbinarized_labels_pkl = open('processedDatasets/labels_binarized.pickle', 'rb')\\n# dump information to that file\\nbinarized_labels_loaded = pickle.load(binarized_labels_pkl)\\n\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "    \n",
    "binarized_labels = sklearn.preprocessing.label_binarize(id_.train_labels, classes = np.array(\"A/B/C/D/E/F/G/H/I/J/K/L/M/N/O/P/Q/R/S/T/U/V/W/X/Y/Z/del/nothing/space\".split(\"/\")))\n",
    "with open('processedDatasets/labels_binarized.pickle', 'wb') as handle:\n",
    "    pickle.dump(binarized_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\"\"\"\n",
    "### test pickle file is working\n",
    "binarized_labels_pkl = open('processedDatasets/labels_binarized.pickle', 'rb')\n",
    "# dump information to that file\n",
    "binarized_labels_loaded = pickle.load(binarized_labels_pkl)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d1f29",
   "metadata": {},
   "source": [
    "### 3. Define NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0251d49d-c31e-4cb1-87e0-767348004c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "### custon CNN, inherits from base torch.nn \n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN,self).__init__(self)  ## override __init__ to be from nn.Module base class\n",
    "        self.conv1 = nn.Conv2d(3,16,5)\n",
    "        self.conv2 = nn.Conv2d(16,32,5)\n",
    "        self.conv3 = nn.Conv2d(32,64,3)\n",
    "        self.conv4 = nn.Conv2d(64,128,5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128,256)\n",
    "        self.fc2 = nn.Linear(256, len(labels_binarized.class))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a720e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
